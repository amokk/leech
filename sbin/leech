#!/bin/sh

# Author: Aleksey Tulinov <aleksey.tulinov@gmail.com>
# Contributor: SumolX <https://github.com/SumolX>

usage() {
    echo "Usage: CONFIG_DIR=\"<DIRECTORY>\" DOWNLOADS_DIR=\"<DIRECTORY>\" $0"
    echo "Or you could omit DOWNLOADS_DIR to download files to current directory."
}

RFC822TOUNIX="$(dirname $0)/rfc822tounix"
CONFIG="$CONFIG_DIR/default"

if [ ! -f $CONFIG ]; then
    echo "ERROR: No config file found at $CONFIG"
    echo
    usage
    exit 1
fi

. $CONFIG

# sanity check
#
if [ ! -f "$XSL_TEMPLATE" ]; then
    echo "ERROR: No XSL template at \"$XSL_TEMPLATE\""
    echo "You probably need to reinstall leech."
    exit 1
fi

if [ ! -f "$FOODS" ]; then
    echo "ERROR: No feeds file at \"$FOODS\""
    echo
    usage
    exit 1
fi

if [ ! -f "$DOWNLOADS" ]; then
    echo "ERROR: No downloads file at \"$DOWNLOADS\""
    echo
    usage
    exit 1
fi

# defaults
#
if [ -z "$DOWNLOADS_DIR" ]; then
    echo "WARNING: DOWNLOADS_DIR is not set. Assuming it's current directory."
    DOWNLOADS_DIR=.
fi

if [ -z "$TMP" ]; then
    TMP=$DOWNLOADS_DIR
fi

if [ -z "$PERSISTENCE" ]; then
    PERSISTENCE=$DOWNLOADS_DIR
fi

if [ -z "$EXPIRATION" ]; then
    EXPIRATION=1 # day
fi

if [ -z "$HISTORY" ]; then
    HISTORY=14 # day
fi

if [ -z "$DOWNLOAD_DELAY" ]; then
    DOWNLOAD_DELAY=5
fi

if [ -z "$TIMEOUT" ]; then
    TIMEOUT=30 # seconds
fi

if [ -z "$RETRY" ]; then
    RETRY=1 # attempts
fi

# temporary file with RSS-feed
LUNCH="$TMP/leech.lunch.$$"

# cURL options
#
CURL_LUNCH_OPTS="-s -f --connect-timeout $TIMEOUT --max-time $TIMEOUT" # cURL options for downloading lunch
CURL_DOWNLOADS_OPTS="$CURL_LUNCH_OPTS -C -" # cURL options for downloading files
# -s for silent, no output
# -f for not outputting failed downloads to files
# --connect_timeout and --max-time for timeouting, default value (30 secs) should be enough for torrent files, otherwise download will continue on next try
# -C - to continue incomplete downloads
#
# Notes:
# --retry doesn't really work with -C - for cURL: http://curl.haxx.se/docs/knownbugs.html

# misc options
#
GREP_OPTS="-i -E" # ignore case, extended regular expressions
SED_OPTS="-r -e"     # enable extended regular expressions
SED_REGEX=".* (.+)\s*\"(.+)\"$" # \1 is URL, \2 is datetime in RFC822
XSLT_OPTS="--novalid" # skip loading DTDs during XSL transformation

# persistence options
#
DB_REGEX="(.+) (.+)" # \1 is md5, \2 is timestamp

# prepare environment
#

# create tmp dir
if [ ! -d "$TMP" ]; then
    echo "WARNING: Temporary directory \"$TMP\" doesn't exist, creating it."
    mkdir -p "$TMP"
    if [ ! $? -eq 0 ]; then
        exit 1
    fi
fi

# remove previous lunch if any
rm -f "$LUNCH"

# create downloads dir
if [ ! -z "$DOWNLOADS_DIR" ] && [ ! -d "$DOWNLOADS_DIR" ]; then
    echo "WARNING: Downloads directory \"$DOWNLOADS_DIR\" doesn't exist, creating it."
    mkdir -p "$DOWNLOADS_DIR"
    if [ ! $? -eq 0 ]; then
        exit 1
    fi
fi

# current time
NOW=$(date -u +%s)

# set expiration in seconds
if [ ! -z "$EXPIRATION" ]; then
    EXPIRATION=$(($EXPIRATION * 86400)) # days * seconds in a day
fi

# set history in seconds
if [ ! -z "$HISTORY" ]; then
    HISTORY=$(($HISTORY * 86400)) # days * seconds in a day
fi

# create persistence dir and db
if [ ! -z "$PERSISTENCE" ]; then
    if [ ! -d "$PERSISTENCE" ]; then
        echo "WARNING: Persistence directory \"$PERSISTENCE\" doesn't exist, creating it"
        mkdir -p "$PERSISTENCE"
        if [ ! $? -eq 0 ]; then
            exit 1
        fi
    fi

    DB="$PERSISTENCE/.leech.db"
fi

# downloading
#
while read FOOD
do
    # skip comments and empty strings
    TAG=$(expr substr "$FOOD" 1 1)
    case $TAG in
        '#')
            continue
            ;;
        '')
            continue
            ;;
    esac

    # download lunch
    #
    echo -n "Downloading feed: $FOOD... "
    curl $CURL_LUNCH_OPTS "$FOOD" >"$LUNCH"
    RET=$?

    # don't parse lunch if download failed
    #
    case $RET in
        0)
            echo "OK"
            ;;

        *)
            echo "Failed: $RET"
            rm -f "$LUNCH"
            continue
            ;;
    esac

    # search lunch for patterns
    #
    while read -r PATTERN # -r to read backslashes
    do
        # skip comments and empty strings
        TAG=$(expr substr "$PATTERN" 1 1)
        case $TAG in
            '#')
                continue
                ;;
            '')
                continue
                ;;
        esac

        # download urls if any
        #
        xsltproc $XSLT_OPTS "$XSL_TEMPLATE" "$LUNCH" | grep $GREP_OPTS "$PATTERN" | while read STR
        do
            URL=$(echo $STR | sed $SED_OPTS "s/$SED_REGEX/\1/")

            if [ -z "$URL" ]; then
                continue
            fi

            # timestamp in RSS is in RFC822 format, it need to be converted to string understandable by `date`
            TIMESTAMP=$(echo $STR | sed $SED_OPTS "s/$SED_REGEX/\2/")
            UNIXTIME=$($RFC822TOUNIX "$TIMESTAMP")
            if [ ! $? -eq 0 ]; then
                echo "WARNING: RSS timestamp ($TIMESTAMP) can't be parsed correctly, expiration feature might not work properly"
                UNIXTIME=""
            fi

            # if expiration set, check RSS entry pub date
            if [ ! -z "$UNIXTIME" ] && [ $(($NOW - $UNIXTIME)) -gt $EXPIRATION ]; then
                echo "Skipping $URL: expired"
                continue
            fi

            MD5=$(expr substr "$(echo -n "$URL" | md5sum)" 1 32) #"

            # if persistence enabled - check md5 of URL for duplicates
            if [ -f "$DB" ]; then
                grep "$MD5" "$DB" 2>&1 >/dev/null
                if [ $? -eq 0 ]; then
                    echo "Skipping $URL: already downloaded"
                    continue
                fi
            fi

            echo -n "Downloading: $URL... "

            sleep $DOWNLOAD_DELAY

            if [ ! -z "$FORCE_SUFFIX" ]; then
                FILENAME="$MD5$FORCE_SUFFIX"
                ADDITIONAL_CURL_OPTS="-o $FILENAME"
            else # default opts
                ADDITIONAL_CURL_OPTS="-O -J"
                # -O to extract filename from URL
                # -J to extract filename from server response
            fi

            (cd $DOWNLOADS_DIR && exec curl $CURL_DOWNLOADS_OPTS $ADDITIONAL_CURL_OPTS "$URL")
            RET=$?

            case $RET in
                0)
                    echo "OK"
                    ;;
                *)
                    echo "Failed: $RET"
                    continue
                    ;;
            esac

            # if persistence enabled, make a record in DB about downloaded file
            if [ ! -z "$DB" ]; then
                echo "$MD5 $NOW" >>"$DB"
            fi
        done
    done <"$DOWNLOADS"

    # cleanup
    #
    rm -f "$LUNCH"
done <"$FOODS"

# delete old records from DB
#
if [ ! -z "$DB" ] && [ -f "$DB" ]; then
    DB_TMP="$DB.tmp"
    rm -f "$DB_TMP" && touch "$DB_TMP"

    # write new database to tmp file
    while read LINE
    do
        TIMESTAMP=$(echo "$LINE" | sed $SED_OPTS "s/$DB_REGEX/\2/")
        if [ $(($NOW - $TIMESTAMP)) -gt $HISTORY ]; then
            continue
        fi

        echo "$LINE" >>"$DB_TMP"
    done <"$DB"

    # replace db with new one
    if [ -f "$DB_TMP" ]; then
        mv "$DB_TMP" "$DB"
    fi
fi
